<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on Bo&#39;s Blog</title>
    <link>https://bochang.me/blog/tags/machine-learning/</link>
    <description>Recent content in machine learning on Bo&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Aug 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://bochang.me/blog/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Importance Weighted Autoencoders and Jackknife Methods</title>
      <link>https://bochang.me/blog/posts/iwae/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://bochang.me/blog/posts/iwae/</guid>
      <description>

&lt;p&gt;This blog post is a summary of two papers: Burda et al. (2015) and Nowozin (2018).
We first give a quick overview of variational inference and variational autoencoders (VAE), which approximate the posterior distribution by a simpler distribution and maximize the evidence lower bound (ELBO).
Blei et al. (2017) and Zhang et al. (2018) are among some excellent survey papers on variational inference.
Next, the importance weighted autoencoder (IWAE) is introduced, and its properties are presented.
Finally, we describe the jackknife variational inference (JVI) as a way to reduce the bias of IWAE estimators.&lt;/p&gt;

&lt;h2 id=&#34;variational-autoencoders&#34;&gt;Variational autoencoders&lt;/h2&gt;

&lt;p&gt;Consider a Bayesian model involving the (set of) latent variable $z$ and  observation $x$, where the joint density can be decomposed into
$$
p(x, z) = p(x | z) p(z),
$$
where $p(x | z)$ is the likelihood and $p(z)$ is the prior distribution of the latent variable $z$.
The posterior distribution $p(z|x)$ is of central interest in Bayesian inference.
However, it is often intractable, and approximate inference is required.&lt;/p&gt;

&lt;p&gt;Variational inference aims to approximate the posterior distribution by a variational distribution and to derive a lower bound of the marginal log-likelihood of data $\log p(x)$.
Variational autoencoder (VAE) is a type of amortized variational inference method.
Here, &amp;ldquo;amortized&amp;rdquo; means that the variational distribution $q(z|x)$ is parametrized by a function of $x$, whose parameters are shared across all observations.&lt;/p&gt;

&lt;p&gt;We first rewrite the marginal log-likelihood by introducing the variational distribution $q(z|x)$:
\begin{align}
\log p(x) &amp;amp;= \log \int p(x|z) p(z) \mathrm{d} z
\newline
&amp;amp;= \log \int \frac{p(x|z) p(z)}{q(z|x)} q(z|x) \mathrm{d} z
\newline
&amp;amp;= \log \mathbb{E}_{q(z|x)} \left[ \frac{p(x|z) p(z)}{q(z|x)} \right].
\label{eq:marginal}
\tag{1}
\end{align}
If we pull the expectation out of the logarithm function, which is concave, &lt;a href=&#34;https://en.wikipedia.org/wiki/Jensen%27s_inequality&#34;&gt;Jensen&amp;rsquo;s inequality&lt;/a&gt; gives the following inequality:
$$
\log p(x) \ge
\mathbb{E}_{q(z|x)}
\log \left[ \frac{p(x|z) p(z)}{q(z|x)} \right].
\label{eq:elbo}
\tag{2}
$$
The right-hand side is called the evidence lower bound (ELBO), denoted by $\mathcal{L}$.
The inference problem then becomes an optimization problem that tries to find a variational distribution $q(z|x)$ that maximizes $\mathcal{L}$.&lt;/p&gt;

&lt;p&gt;There are at least three ways of rewriting the ELBO:
\begin{align}
\mathcal{L} &amp;amp;=
\log p(x) - D_{\mathrm{KL}} ( q(z|x) \parallel p(z|x) )
\newline
&amp;amp;=
\mathbb{E}_{q(z|x)} \log p(x,z) + \mathbb{E}_{q(z|x)} [-\log q(z|x)]
\newline
&amp;amp;=
\mathbb{E}_{q(z|x)} \log p(x|z)
- D_{\mathrm{KL}} ( q(z|x) \parallel p(z) )
\end{align}&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The first equation shows that the difference between the marginal log-likelihood $\log p(x)$ and the ELBO $\mathcal{L}$ is the KL divergence between $q(z|x)$ and $p(z|x)$.
When these two distributions are identical (almost everywhere), the marginal log-likelihood equals the ELBO.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;In the second equation, the first term represents the &amp;ldquo;energy&amp;rdquo; and the second term represents the entropy of $q(z|x)$. The energy term encourages $q(z|x)$ to focus probability mass on where the joint probability $p(x, z)$ is large. The entropy encourages $q(z|x)$ to spread the probability mass to avoid concentrating on one location.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;The third equation is a more explicit representation of the standard architecture of a variational autoencoder.
The variational distribution $q(z|x)$ and the likelihood function $p(x|z)$ are represented by an encoder network and a decoder network, respectively.
Furthermore, $q(z|x)$ and $p(z)$ are often assumed to be multivariate independent Gaussian so that their KL divergence is of closed-form.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A simple Monte Carlo estimator of the ELBO $\mathcal{L}$ approximates the expectation in Equation \ref{eq:elbo} by the sample mean.
Let $z_i$, for $i=1, \ldots, k$, be independent samples drawn from $q(z|x)$, then the estimator is
$$
\widehat{\mathcal{L}}_k^{\mathrm{ELBO}} :=
\frac{1}{k} \sum_{i=1}^k \log \left[ \frac{p(x|z_i) p(z_i)}{q(z_i|x)} \right].
$$
It is obvious that the estimator is unbiased, i.e., $\mathbb{E}_{z_i \sim q(z|x)} \widehat{\mathcal{L}}_k^{\mathrm{ELBO}} = \mathcal{L}$.&lt;/p&gt;

&lt;h2 id=&#34;importance-weighted-autoencoders&#34;&gt;Importance weighted autoencoders&lt;/h2&gt;

&lt;p&gt;What we have done so far is first to define the ELBO $\mathcal{L}$ as a lower bound of $\log p(x)$, and then to estimate it by $\widehat{\mathcal{L}}_k^{\mathrm{ELBO}}$.
An alternative approach is to approximate the expectation (inside the logarithm function) in Equation \ref{eq:marginal} by Monte Carlo, which leads to the importance weighted autoencoders (IWAE) estimator:
$$
\widehat{\mathcal{L}}_k^{\mathrm{IWAE}} :=
\log \left[ \frac{1}{k} \sum_{i=1}^k  \frac{p(x|z_i) p(z_i)}{q(z_i|x)} \right].
$$
Note the difference between $\widehat{\mathcal{L}}_k^{\mathrm{ELBO}}$ and $\widehat{\mathcal{L}}_k^{\mathrm{IWAE}}$.&lt;/p&gt;

&lt;p&gt;If we denote $\mathcal{L}_k := \mathbb{E}_{z_i \sim q(z|x)} \widehat{\mathcal{L}}_k^{\mathrm{IWAE}}$, then by Jensen&amp;rsquo;s inequality,
$$
\mathcal{L}_k
\le \log \left[ \mathbb{E}_{z_i \sim q(z|x)} \frac{1}{k} \sum_{i=1}^k  \frac{p(x|z_i) p(z_i)}{q(z_i|x)} \right]
= \log p(x).
$$
In other words, the expectation of $\widehat{\mathcal{L}}_k^{\mathrm{IWAE}}$ is also a lower bound of $\log p(x)$.
When $k=1$, the ELBO and IWAE estimators are equivalent.
It can be shown that $\mathcal{L}_k$ is tighter than $\mathcal{L}$ when $k&amp;gt;1$:
$$
\mathcal{L} = \mathcal{L}_1 \le \mathcal{L}_2 \le \cdots \le \log p(x),
$$
and
$$
\lim_{k \to \infty} \mathcal{L}_k = \log p(x).
$$
Unsurprisingly, $\widehat{\mathcal{L}}_k^{\mathrm{IWAE}}$ also converges in probability to $\log p(x)$ as $k\to \infty$.
A more detailed asymptotic analysis shows that
$$
\mathcal{L}_k
= \log p(x) - \frac{\mu_2}{2 \mu^2} \frac{1}{k}
+ \left( \frac{\mu_3}{3\mu^2} - \frac{3\mu_2^2}{4\mu^4} \right) \frac{1}{k^2}
+ O(k^{-3}),
$$
where $\mu$ and $\mu_j$ are the expectation and the $j$-th central moment of $p(x|z_i) p(z_i) / q(z_i|x)$ with $z_i \sim q(z|x)$, respectively.&lt;/p&gt;

&lt;p&gt;An interesting perspective on the IWAE is that, $\widehat{\mathcal{L}}_k^{\mathrm{IWAE}}$ can be regarded as an estimator of $\log p(x)$.
As shown above, the estimator is &lt;a href=&#34;https://en.wikipedia.org/wiki/Consistent_estimator&#34;&gt;consistent&lt;/a&gt; but &lt;a href=&#34;https://en.wikipedia.org/wiki/Bias_of_an_estimator&#34;&gt;biased&lt;/a&gt;, and the bias is in the order of $O(k^{-1})$.
The remaining sections try to reduce the bias to a higher/smaller order so that the estimator is closer to the marginal log-likelihood when $k$ is large.&lt;/p&gt;

&lt;h2 id=&#34;jackknife-resampling&#34;&gt;Jackknife resampling&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Jackknife_resampling&#34;&gt;jackknife&lt;/a&gt; is a resampling technique that can be used to estimate the bias of an estimator and further to reduce the bias.
Let $\widehat{T}_n$ be a consistent but biased estimator of $T$, evaluated on $n$ samples.
Assume the expectation $\mathbb{E} (\widehat{T}_n)$ can be written as an asymptotic expansion as $n \to \infty$:
$$
\mathbb{E} (\widehat{T}_n) = T + \frac{a_1}{n} + \frac{a_2}{n^2} + O(n^{-3}).
$$
Then the bias of $\widehat{T}_n$ is in the order of $O(n^{-1})$.&lt;/p&gt;

&lt;p&gt;A debiased estimator $\widetilde{T}_{n,1}$ can be defined as follows:
$$
\widetilde{T}_{n,1} := n \widehat{T}_n - (n-1) \widehat{T}_{n-1}.
$$
The idea is that the first order term is canceled by calculating the difference.
\begin{align}
\mathbb{E} (\widetilde{T}_{n,1})
&amp;amp;= n \mathbb{E} (\widehat{T}_n) - (n-1) \mathbb{E} (\widehat{T}_{n-1})
\newline
&amp;amp;= n \left( T + \frac{a_1}{n} + \frac{a_2}{n^2} + O(n^{-3}) \right)
\newline
&amp;amp;\qquad - (n-1) \left( T + \frac{a_1}{n-1} + \frac{a_2}{(n-1)^2} + O(n^{-3}) \right)
\newline
&amp;amp;= T + \frac{a_2}{n} - \frac{a_2}{n-1} + O(n^{-2})
\newline
&amp;amp;= T + O(n^{-2}).
\end{align}
The bias of $\widetilde{T}_{n,1}$ is in the order of $O(n^{-2})$ instead of $O(n^{-1})$.
When $n$ is large, $\widetilde{T}_{n,1}$ has a lower bias than $\widehat{T}_n$.&lt;/p&gt;

&lt;p&gt;The estimator $\widehat{T}_{n-1}$ can be calculated on any $n-1$ samples. In practice, given $n$ samples, it is evaluated on the $n$ &amp;ldquo;leave-one-out&amp;rdquo; subsets of size $n-1$, and the average of the $n$ estimates is used in place of $\widehat{T}_{n-1}$, which reduces the variance of the estimator.&lt;/p&gt;

&lt;p&gt;The above debiasing method can be further generalized to higher orders.
For example, let
$$
\widetilde{T}_{n,2}
:= \frac{n^2}{2} \widehat{T}_n
- (n-1)^2 \widehat{T}_{n-1}
+ \frac{(n-2)^2}{2} \widehat{T}_{n-2},
$$
then
$$
\mathbb{E} ( \widetilde{T}_{n,2} )
= T + O(n^{-3}),
$$
that is, the bias of $\widetilde{T}_{n,2}$ is in the order of $O(n^{-3})$.
More generally, for
$$
\widetilde{T}_{n,m}
:= \sum_{j=0}^m c(n, m, j) \widehat{T}_{n-j},
$$
where
$$
c(n, m, j) = (-1)^j \frac{(n-j)^m}{(m-j)! j!},
$$
the bias is in the order of $O(n^{-(m+1)})$.&lt;/p&gt;

&lt;h2 id=&#34;jackknife-variational-inference&#34;&gt;Jackknife variational inference&lt;/h2&gt;

&lt;p&gt;The application of the jackknife method to the IWAE estimator should be straightforward.
The jackknife variational inference (JVI) estimator is defined as follows:
$$
\widehat{\mathcal{L}}_{k,1}^{\mathrm{JVI}}
:= k \widehat{\mathcal{L}}_k^{\mathrm{IWAE}}
- (k-1) \widehat{\mathcal{L}}_{k-1}^{\mathrm{IWAE}},
$$
and more generally,
$$
\widehat{\mathcal{L}}_{k,m}^{\mathrm{JVI}}
:= \sum_{j=0}^m c(k, m, j) \widehat{\mathcal{L}}_{k-j}^{\mathrm{IWAE}}.
$$
The bias of $\widehat{\mathcal{L}}_{k,m}^{\mathrm{JVI}}$, as an estimator of $\log p(x)$, is thus in the order of $O(k^{-(m+1)})$.&lt;/p&gt;

&lt;p&gt;Again, the IWAE estimator $\widehat{\mathcal{L}}_{k-j}^{\mathrm{IWAE}}$ can be evaluated on a single subset of samples of size $k-j$, or by the average of that on all subsets of size $k-j$.
In the latter case, the computational cost is significant since $\sum_{j=0}^m {k \choose j}$ could be large; the time complexity is bounded by
$$
O \left( k e^m \left( \frac{k}{m} \right)^m \right).
$$
In practice, the algorithm is feasible only for small values of $m$.
Other variations of JVI are also provided by Nowozin (2018), at the cost of higher variance of the estimator.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Blei, D. M., Kucukelbir, A., &amp;amp; McAuliffe, J. D. (2017). Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518), 859-877.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Burda, Y., Grosse, R., &amp;amp; Salakhutdinov, R. (2015). Importance weighted autoencoders. International Conference on Learning Representations.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nowozin, S. (2018). Debiasing evidence approximations: On importance-weighted autoencoders and jackknife variational inference. International Conference on Learning Representations.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Zhang, C., Butepage, J., Kjellstrom, H., &amp;amp; Mandt, S. (2018). Advances in variational inference. IEEE transactions on pattern analysis and machine intelligence.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Bandits and UCB Algorithm</title>
      <link>https://bochang.me/blog/posts/bandits/</link>
      <pubDate>Sat, 08 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://bochang.me/blog/posts/bandits/</guid>
      <description>

&lt;p&gt;In our recent paper &lt;em&gt;Vine Copula Structure Learning via Monte Carlo Tree Search&lt;/em&gt; (AISTATS 2019), we apply the UCT (Upper Confidence bounds applied to Trees) algorithm to find an approximate solution to an NP-hard structure learning problem in statistics.
The UCT algorithm is based on the UCB1 (Upper Confidence Bound) algorithm for the stochastic bandit problem.&lt;/p&gt;

&lt;p&gt;Thankfully, I audited &lt;a href=&#34;https://www.cs.ubc.ca/~nickhar/&#34;&gt;Prof. Nick Harvey&lt;/a&gt;&amp;rsquo;s &lt;a href=&#34;https://www.cs.ubc.ca/~nickhar/F18-531/&#34;&gt;learning theory course&lt;/a&gt; this semester.
In one lecture, he gave a clear exposition of the multi-armed bandit problem and algorithms.
It deepened my understanding of the theoretical properties of the UCB1 algorithm.
This blog post is mostly a transcription of the lecture; it gives an overview of the regret bound analysis of the explore-first algorithm and UCB1 algorithm. The material is also based on the first chapter of Slivkins (2017).&lt;/p&gt;

&lt;h2 id=&#34;1-problem-formulation&#34;&gt;1. Problem formulation&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Multi-armed_bandit&#34;&gt;multi-armed bandit problem&lt;/a&gt; has many practical applications including clinical trials, financial portfolio design, and online advertising.
The name comes from imagining a gambler at a row of slot machines (sometimes known as &amp;ldquo;one-armed bandits&amp;rdquo;), each with a lever.
In each round, the gambler picks a machine to play, then the reward for the chosen machine is observed.
The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls.
In the rest of the post, we use the term &amp;ldquo;arm&amp;rdquo; as a synonym for a slot machine.&lt;/p&gt;

&lt;p&gt;In this post, we focus on the &lt;strong&gt;stochastic bandit problem&lt;/strong&gt;, where the rewards for each arm are i.i.d. from a probability distribution specific to that arm.
Another variant of the multi-armed bandit problem is called the &lt;strong&gt;adversarial bandit problem&lt;/strong&gt;, where an adversary chooses the reward for each arm while the player chooses an arm.&lt;/p&gt;

&lt;p&gt;The fundamental tradeoff the gambler faces is between &lt;strong&gt;exploration&lt;/strong&gt; and &lt;strong&gt;exploitation&lt;/strong&gt;.
In machine learning, exploration stands for the acquisition of new knowledge, and exploitation refers to an optimized decision based on existing knowledge.
In the case of the multi-armed bandit problem, the gambler needs to decide between trying different arms to get more information about their rewards and playing the arm that has the highest average reward so far.&lt;/p&gt;

&lt;p&gt;The following notations are used throughout the post.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(K\) is the total number of arms and \(T\) is the total number of rounds, both are known in advance. Arms are denoted by \(a \in [K]\), rounds by \(t \in [T]\). (The bracket notation \([n]\) denotes the first \(n\) positive integers \([n] := \{1, 2, \ldots, n\}\).)&lt;/li&gt;
&lt;li&gt;The reward for arm \(a\) is i.i.d. from \(\mathcal{D}_a\), which is a distribution supported on \([0,1]\). The expected reward of arm \(a\) is denoted by \(\mu(a) := \int_0^1 x \, \mathrm{d}\mathcal{D}_a(x)\).&lt;/li&gt;
&lt;li&gt;The best expected reward is denoted by \(\mu^* := \max_{a \in [K]} \mu(a)\), and the best arm is \(a^* = \operatorname{argmax}_{a \in [K]} \mu(a)\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The (cumulative) &lt;strong&gt;regret&lt;/strong&gt; in round \(t\) is defined as
$$ R(t) =  \mu^* t  - \sum_{s=1}^t \mu(a_s), $$
where \(a_s\) is the chosen arm in round \(s\).
The regret is the difference between the sum of rewards associated with the optimal arm and the sum of expected rewards by an algorithm.
The goal of the algorithm is to minimize regret.
Note that \(a_s\) is a random quantity, since it may depend on the randomness in rewards and/or the algorithm.
Therefore the regret \(R(t)\) is also random and we are interested in the expected regret \(\mathbb{E}[R(t)]\) or \(\mathbb{E}[R(T)]\).&lt;/p&gt;

&lt;p&gt;It may be a good time now to review &lt;a href=&#34;https://en.wikipedia.org/wiki/Hoeffding%27s_inequality&#34;&gt;Hoeffding&amp;rsquo;s inequality&lt;/a&gt;, an important concentration inequality widely used in machine learning theory.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Hoeffding&amp;rsquo;s inequality&lt;/strong&gt;&lt;br /&gt;
Let \(X_1, \ldots, X_n\) be independent random variables and let \(\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i\). Assume \(0 \leq X_i \leq 1\) almost surely. Then, for any \(t&amp;gt;0\),
$$\mathbb{P}(|\bar{X} - \mathbb{E}[\bar{X}]| &amp;gt; t) \leq 2 \exp (-2nt^2).$$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;2-explore-first-algorithm&#34;&gt;2. Explore-first algorithm&lt;/h2&gt;

&lt;p&gt;A simple idea is to explore arms uniformly and pick an empirically best arm for exploitation.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Explore-first algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Exploration phase:     Try each arm \(N\) times. Let \(\bar\mu(a)\) be the average reward for arm \(a\);&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Exploitation phase: Select arm \(\hat{a}\) with the highest average reward \(\hat{a} = \operatorname{argmax}_{a \in [K]} \bar\mu(a)\). Play arm \(\hat{a}\) in all remaining rounds.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;The parameter \(N\) is fixed in advance; it will be chosen later as a function of \(T\) and \(K\).&lt;/p&gt;

&lt;h3 id=&#34;2-1-regret-bound&#34;&gt;2.1 Regret bound&lt;/h3&gt;

&lt;p&gt;Our goal is to give an upper bound of the expected regret \(\mathbb{E}[R(T)]\) as a function of \(K\) and \(T\):
$$
\mathbb{E}[R(T)]=O \big(T^{2 / 3} (K \log T)^{1 / 3} \big).
$$&lt;/p&gt;

&lt;p&gt;To facilitate our analysis, we define the &lt;strong&gt;clean event&lt;/strong&gt; \(C\) to be the event that \(\bar\mu(a)\) is close to \(\mu(a)\) for all arms:
$$
C = \{ |\bar\mu(a) - \mu(a)| \leq r, \forall a \in [K] \},
$$
where \(r\) is called the &lt;strong&gt;confidence radius&lt;/strong&gt;.
The &lt;strong&gt;bad event&lt;/strong&gt; \(\bar{C}\) is the complement of the clean event.
By the &lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_total_expectation&#34;&gt;law of total expectation&lt;/a&gt;,
$$
\mathbb{E}[R(T)] =
\mathbb{E}[R(T) | C]
\mathbb{P}[C]
+
\mathbb{E}[R(T) | \bar{C}]
\mathbb{P}[\bar{C}].
$$&lt;/p&gt;

&lt;p&gt;First of all, we want to make sure \(\mathbb{P}[\bar{C}]\) is small.
In other words, the average reward \(\bar\mu(a)\) should be a good estimate of the true expected reward \(\mu(a)\).
By Hoeffding&amp;rsquo;s inequality, the deviation of the average reward from the true expected reward can be quantified as follows:&lt;/p&gt;

&lt;p&gt;$$
\mathbb{P}( |\bar\mu(a) - \mu(a)| &amp;gt; r) \leq 2 \exp (-2Nr^2), \quad \forall a \in [K].
$$&lt;/p&gt;

&lt;p&gt;We want to bound this probability by a sufficiently small number, say \(2/T^4\). This can be achieved by setting the confidence radius \(r = \sqrt{\frac{2 \log T}{N}}\):
$$
\mathbb{P}( |\bar\mu(a) - \mu(a)| &amp;gt; r) \leq \frac{2}{T^4}, \quad \forall a \in [K].
$$
Note that the choice of \(T^4\) is somewhat arbitrary; the exponent only affects the multiplicative constant of \(r\).&lt;/p&gt;

&lt;p&gt;Using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Boole%27s_inequality&#34;&gt;union bound&lt;/a&gt;, we can find an upper bound of the probability of the bad event.
$$
\begin{align}
\mathbb{P}(\bar{C})
&amp;amp;=
\mathbb{P} \big(\{ \exists a \in [K] \ \text{s.t.}\ |\bar\mu(a) - \mu(a)| &amp;gt; r \} \big)
\newline
&amp;amp;=
\mathbb{P} \big(\bigcup_{a \in [K]}\{|\bar\mu(a) - \mu(a)| &amp;gt; r \} \big)
\newline
&amp;amp;\leq \sum_{a \in [K]} \mathbb{P} (|\bar\mu(a) - \mu(a)| &amp;gt; r )
\newline
&amp;amp;\leq
\frac{2K}{T^4}
\newline
&amp;amp;\leq
\frac{2}{T^3}.
\end{align}
$$
The last inequality is because each arm is explored at least once, i.e., \(T \geq K\).&lt;/p&gt;

&lt;p&gt;Next, we focus on \(\mathbb{E}[R(T) | C]\).
By definition, \(\bar\mu(\hat{a}) \geq \bar\mu(a^*)\).
Given the clean event happens,
an upper bound of \(\mu(a^*) - \mu(\hat{a}) \) can be obtained:
$$
\begin{align}
\mu(a^*) - \mu(\hat{a})
&amp;amp;\leq
\mu(a^*) - \mu(\hat{a}) + \bar\mu(\hat{a}) - \bar\mu(a^*)
\newline
&amp;amp;=
\big(\mu(a^*) - \bar\mu(a^*)\big) + \big(\bar\mu(\hat{a}) - \mu(\hat{a}) \big)
\newline
&amp;amp;\leq 2r.
\end{align}
$$
Intuitively, it indicates that conditioning on the clean event, the chosen arm \(\hat{a}\) cannot be much worse than \(a^*\).&lt;/p&gt;

&lt;p&gt;There are \(NK\) rounds in the exploration phase, and each round has at most regret of 1; there are \(T - NK\) rounds in the exploitation phase and the regret incurred in each round is bounded by \(2r\).
As a result, the regret in round \(T\) can be bounded by
$$
\begin{align}
\mathbb{E}[R(T) | C]
&amp;amp;\leq NK + (T - NK) 2r
\newline
&amp;amp;\leq NK + 2Tr
\newline
&amp;amp;= NK + 2T \sqrt{\frac{2 \log T}{N}}.
\end{align}
$$
Since we can choose the number of rounds in the exploration phase,
the right-hand side can be minimized by setting
\( N = (T/K)^{2 / 3} (2\log T)^{1 / 3} \).
Therefore,
$$
\mathbb{E}[R(T) | C]
\leq
T^{2 / 3} (2K\log T)^{1 / 3}.
$$&lt;/p&gt;

&lt;p&gt;So far, we have&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\mathbb{E}[R(T) | C]
\leq
T^{2 / 3} (2K \log T)^{1 / 3}
\);&lt;/li&gt;
&lt;li&gt;\( \mathbb{P}[C] \leq 1 \);&lt;/li&gt;
&lt;li&gt;\( \mathbb{E}[R(T) | \bar{C}] \leq T \), since there are in total \(T\) rounds, each round incurs at most regret of 1;&lt;/li&gt;
&lt;li&gt;\(\mathbb{P}[\bar{C}] \leq 2 / T^4 \).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Putting everything together,
$$
\begin{align}
\mathbb{E}[R(T)] &amp;amp;=
\mathbb{E}[R(T) | C]
\mathbb{P}[C]
+
\mathbb{E}[R(T) | \bar{C}]
\mathbb{P}[\bar{C}]
\newline
&amp;amp;\leq
T^{2 / 3} (2K\log T)^{1 / 3}
+
T \cdot \frac{2}{T^3}
\newline
&amp;amp;= O \big(T^{2 / 3} (K \log T)^{1 / 3} \big).
\end{align}
$$&lt;/p&gt;

&lt;h2 id=&#34;3-upper-confidence-bound-algorithm&#34;&gt;3. Upper confidence bound algorithm&lt;/h2&gt;

&lt;p&gt;The problem with the explore-first algorithm is that each arm is explored for the same number of rounds, which causes inefficiency.
In other words, the exploration schedule should depend on the history of the observed rewards.
Instead of using the same confidence radius for any arm in any round, we denote the confidence radius for arm \(a\) at time \(t\) by \(r_t(a)\).
Let \(n_t(a)\) be the number of times arm \(a\) is selected in rounds \(1, 2, \ldots, t\) and \(\bar\mu_t(a)\)  be the average reward of arm \(a\) up to time \(t\).
The &lt;strong&gt;upper confidence bound&lt;/strong&gt; is defined as
$$
\mathrm{UCB}_t(a) = \bar\mu_t(a) + r_t(a),
$$
where \( r_t(a) = \sqrt{\frac{2 \log T}{n_t(a)}} \).&lt;/p&gt;

&lt;p&gt;The UCB1 algorithm chooses the best arm based on an optimistic estimate. The algorithm is as follows.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;UCB1 algorithm&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Try each arm once;&lt;/li&gt;
&lt;li&gt;In round \(t\), pick \( a_t = \operatorname{argmax}_{a \in [K]} \mathrm{UCB}_t(a)\).&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Unlike the explore-first algorithm, there is no clear exploration/exploitation phase. However, the definition of the upper confidence bound manifests the exploration-exploitation tradeoff: \(\bar\mu_t(a)\) encourages the exploitation of high reward arms, while \(r_t(a)\) encourages the exploration of less played arms.&lt;/p&gt;

&lt;h3 id=&#34;3-1-regret-bound&#34;&gt;3.1 Regret bound&lt;/h3&gt;

&lt;p&gt;The regret bound of the UCB1 algorithm is
$$
\mathbb{E}[R(t)] = O \big(\sqrt{Kt \log T} \big),
\quad
\forall t \in [T].
$$
The idea of the analysis is the same as before: define a clean event \(C\), obtain upper bounds of \(\mathbb{P}[\bar{C}]\) and \(\mathbb{E}[R(t)|C]\), and finally bound \(\mathbb{E}[R(t)]\).&lt;/p&gt;

&lt;p&gt;Since the confidence radius now depends on \(t\) as well, we need a more refined definition of the &lt;strong&gt;clean event&lt;/strong&gt;:
$$
C = \{|\bar\mu_t(a) - \mu(a)| \leq r_t(a), \forall a \in [K], \forall t \in [T] \}.
$$
Applying Hoeffding&amp;rsquo;s inequality and a union bound (and ignoring some technicalities),
$$
\mathbb{P}[\bar{C}] \leq \frac{2KT}{T^4} \leq \frac{2}{T^2}.
$$&lt;/p&gt;

&lt;p&gt;Now we focus on \(\mathbb{E}[R(t)|C]\) and assume the clean event holds.
By definition, \( \mathrm{UCB}_t(a_t) \geq \mathrm{UCB}_t(a^*) \) for any round \(t \in [T]\).
As a result,&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
\mu(a^*) - \mu(a_t)
&amp;amp;\leq
\mu(a^*) - \mu(a_t) + \mathrm{UCB}_t(a_t) - \mathrm{UCB}_t(a^*)
\newline
&amp;amp;= \big(\mu(a^*) - \mathrm{UCB}_t(a^*)\big) + \big(\mathrm{UCB}_t(a_t) - \mu(a_t) \big),
\end{align}
$$
where
$$
\mu(a^*) - \mathrm{UCB}_t(a^*)
=\mu(a^*) - \bar\mu_t(a^*) - r_t(a^*)
\leq 0,
$$
and
$$
\mathrm{UCB}_t(a_t) - \mu(a_t)
=\bar\mu_t(a_t) - \mu(a_t) + r_t(a_t)
\leq 2 r_t(a_t).
$$
Therefore,
$$
\mu(a^*) - \mu(a_t)
\leq
2 r_t(a_t)
= 2 \sqrt{\frac{2 \log T}{n_t(a_t)}}.
$$&lt;/p&gt;

&lt;p&gt;For each arm \(a\) and a given time \(t \in [T]\), consider the last round \(t_0 \leq t\) when this arm is chosen.
Since arm \(a\) is never played between round \(t_0\) and round \(t\), we have \(n_{t_0}(a) = n_t(a)\).
Applying the above inequality to arm \(a\) and round \(t_0\),
$$
\mu(a^*) - \mu(a) \leq 2 \sqrt{\frac{2 \log T}{n_t(a)}},
\quad
\forall t \in [T].
$$
Intuitively it means that, under the clean event, if an arm is selected many times, it cannot be much worse than the best arm; or equivalently, if an arm is much worse than the best arm, it won&amp;rsquo;t be selected many times.&lt;/p&gt;

&lt;p&gt;The regret in round \(t\) is thus bounded by
$$
R(t) = \sum_{a \in [K]}
n_t(a) \big(\mu(a^*) - \mu(a)\big)
\leq
2 \sqrt{2 \log T}
\sum_{a \in [K]}
\sqrt{n_t(a)}.
$$
Since the square root function is concave, by &lt;a href=&#34;https://en.wikipedia.org/wiki/Jensen%27s_inequality&#34;&gt;Jensen&amp;rsquo;s inequality&lt;/a&gt;,
$$
\sum_{a \in [K]}
\sqrt{n_t(a)}
= K \sum_{a \in [K]}
\frac{1}{K} \sqrt{n_t(a)}
\leq
K
\sqrt{\frac{1}{K}\sum_{a \in [K]} n_t(a)}
= \sqrt{Kt}.
$$
Therefore,
$$
\mathbb{E}[R(t)|C] \leq 2 \sqrt{2 Kt \log T}.
$$&lt;/p&gt;

&lt;p&gt;Finally, much like what we did for the explore-first algorithm, the expected regret in round \(t\) can be bounded by
$$
\begin{align}
\mathbb{E}[R(t)] &amp;amp;=
\mathbb{E}[R(t) | C]
\mathbb{P}[C]
+
\mathbb{E}[R(t) | \bar{C}]
\mathbb{P}[\bar{C}]
\newline
&amp;amp;\leq
2 \sqrt{2 Kt \log T}
+
t \cdot \frac{2}{T^2}
\newline
&amp;amp;= O \big(\sqrt{Kt \log T} \big),
\quad
\forall t \in [T].
\end{align}
$$&lt;/p&gt;

&lt;h3 id=&#34;3-2-an-instance-dependent-regret-bound&#34;&gt;3.2 An instance-dependent regret bound&lt;/h3&gt;

&lt;p&gt;We can also obtain another regret bound using the inequality
$$
\mu(a^*) - \mu(a) \leq 2 \sqrt{\frac{2 \log T}{n_T(a)}}.
$$
Rearrange the terms,
$$
n_T(a) \leq \frac{8 \log T}{(\mu(a^*) - \mu(a))^2},
\quad
\text{if }
\mu(a) &amp;lt; \mu(a^*).
$$
The regret in round \(T\) is bounded by
$$
R(T) = \sum_{a \in [K]} n_T(a) \big(\mu(a^*) - \mu(a)\big)
\leq
8 \log T
\sum_{\{a \in [K]: \mu(a) &amp;lt; \mu(a^*)\}}
\frac{1}{\mu(a^*) - \mu(a)}.
$$
Therefore,
$$
\mathbb{E}[R(T)]
\leq
O(\log T)
\sum_{\{a \in [K]: \mu(a) &amp;lt; \mu(a^*)\}}
\frac{1}{\mu(a^*) - \mu(a)}.
$$&lt;/p&gt;

&lt;p&gt;This regret bound is logarithmic in \(T\), with a constant that can be arbitrarily large depending on a problem instance.
This type of regret bound is called &lt;strong&gt;instance-dependent&lt;/strong&gt;. The existence of a logarithmic regret bound is a benefit of the UCB1 algorithm compared to the explore-first algorithm, whose regret bound is polynomial in \(T\).&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Slivkins, A. (2017). Introduction to Multi-Armed Bandits. &lt;a href=&#34;http://slivkins.com/work/MAB-book.pdf&#34;&gt;http://slivkins.com/work/MAB-book.pdf&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>